---
layout: post
title:  "Variational Autoencoders - Part I The Fundamentals"
date:   2018-11-01 18:58:12 +1000
categories: generative_models
comments: false
---

{% include mathjax.html %}

### Introduction

### The Infinite Mixture Distribution

We will start our exploration by considering the following generative model

$$
  p(x) = \int_{z}p(x|z)p(z)
$$

where $$ p(z) $$ is a continuous distribution and the form of $$ p(x|z) $$ is dependent
on the data we would like to model. Lets consider the case where $$ p(z) = \mathcal{N}(0, I) $$,
and $$ x $$ is some binary random variable $$ p(x|z) = \mathcal{B}(1, \alpha) $$

If you have studied mixture distributions before (see my [previous post] for an
introduction), the form of this equation will look quite familiar. This is because
we have just defined an infinite mixture of Bernoulli distributions! This can be
realised by noticing that for every value $$ z $$ of our continuous prior have a
separate conditional distribution $$ x ~ p(x|z) $$.

For completeness, the graphical model of this equation is:

![Generative]({{ "/assets/images/generative.png" }})

### Monte-Carlo Integration


### References

1.

{% if page.comments %}
{% include disqus.html %}
{% endif %}

[pytorch]: https://pytorch.org
[kmeans]: https://en.wikipedia.org/wiki/K-means_clustering
[github]: https://github.com/angusturner/generative_models/
[popgun]: http://popgun.ai/
[variational autoencoders]: https://arxiv.org/abs/1312.6114
[twitter]: https://twitter.com/AngusTurner9
[previous post]: https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html
