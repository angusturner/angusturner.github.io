---
layout: post
title:  "Diffusion Probabilistic Models - Part I"
date:   2021-06-29
categories: generative_models
comments: false
---

{% include mathjax.html %}
{% include styles.html %}

### Introduction

Recently I have been studying a class of generative models called diffusion models. This line of research
grabbed my attention with the release of "Denoising Diffusion Probabilistic Models" (Ho et al, 2020), which showed
that a CNN trained with a variational lower bound could produce competitive results to a GAN on image modelling. 
As an audio researcher, I was parcticularly excited to these insights translated to audio modelling with the release
of "DiffWave: A Versatile Diffusion Model for Audio Synthesis" (Kong et al, 2020). As well as providing a competitive neural
vocoder, Kong showed that the same model could perform zero-shot denoising and unconditional audio modelling (link to results).

What is fascinating about these models, is that they have deep connections to variational auto-encoding (ref), score-based generative modelling (ref), and 
stochastic differential equations (ref). In this series of blog posts I want to untangle the theory of Ho's model from the perpspective
of variational inference. This is the perspective I am most comfortable with, and I hope that I can build up the theory in an accessible and
intuitive way. Ideally, you will at least be familiar with variational auto-encoders. However I will do a quick refresher.
...


### Variational Autoencoders - A Quick Refresher

Consider the following generative model,

$$
  p(x) = \int_{z}p_{\theta}(x|z)p(z)
$$  

For simplicity assume $$ p(z) = \mathcal{N}(0, I) $$.
Furthermore, we will set $$ p_{\theta}(x|z) $$ as a diagonal gaussian, parameterized by a neural network.  

If you have studied mixture distributions before (see my [previous post] for an
introduction) this equation will look quite familiar. This is because we have just defined an infinite mixture distribution! 
For each value $$ z $$ of the continuous prior distribution, there is a corresponding likelihood $$ p(x|z) $$.

We could try to learn this model directly by maximising:


...

However, it is well known that his approach suffers from high-variance and does not scale to high dimensions. To alleviate this,
we introduce a learned proposal distribution $$ q_{\phi}(z|x) $$, which allows the model to focus on high-probability
regions of the latent space. A typical derivation for this model is as follows:

...


For completeness, we have the following graphical model:

__Figure 1 - Graphical Model for VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/vae.png" }}' height=200px>
<!-- ![VAE]() -->


The key insight of Kingma and Welling was to introduce a low-variance gradient estimator for this model,
by way of the reparameterization trick. Plenty has been written on this already, so I will direct the interested
reader to (...).


### Hierarchical Variational Autoencoders

__Figure 2 - A Hierarchical VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/hierarchical-vae.png" }}' height=200px>

### DDPM as a Restricted Class of Hierarchical VAE

__Figure 2 - Denoising Diffusion Probabilistic Model__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/diffusion.png" }}' height=200px>


### References

{% include citation.html
    no="1"
    authors="Aaron van den Oord, Sander Dieleman, 
    Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu"
    title="Wavenet: A generative model for raw audio"
    year="2016"
    link="https://arxiv.org/abs/1609.03499v2"
%}

{% if page.comments %}
{% include disqus.html %}
{% endif %}

[pytorch]: https://pytorch.org
[kmeans]: https://en.wikipedia.org/wiki/K-means_clustering
[github]: https://github.com/angusturner/generative_models/
[popgun]: http://popgun.ai/
[variational autoencoders]: https://arxiv.org/abs/1312.6114
[twitter]: https://twitter.com/AngusTurner9
[previous post]: https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html
