---
layout: post
title:  "Diffusion Probabilistic Models - Part I"
date:   2021-06-29
categories: generative_models
comments: false
---

{% include mathjax.html %}
{% include styles.html %}

### Introduction

Recently I have been studying a class of generative models known as diffusion probabilistic models. These models were proposed by Jascha Sohl-Dickstein in 2015 (REF), 
but did not catch my attention until last year when Jonathon Ho released "Denoising Diffusion Probabilistic Models" (DDPM) (REF). Building on the work of Sohl-Dickstein,
Ho presented a model that can match or surpass GANs on image generation, while using a stable objective based on an approximate likelihood - no adversarial objective 
required. 

Since the release of DDPM, there has been a torrent of renewed interest in diffusion models. Related works have extended their success to the domain
of audio modelling (REF), text-to-speech (REF), and multivariate time-series forecasting (REF). What is fascinating about these models,
is their deep connections to variational autoencoding (REF), neural compression (REF), score-based generative modelling (REF), 
and stochastic differential equations (REF). 

In this blog I want to focus on building up the intuition of diffusion models, as they are presented in DDPM and a few select related works. 
I want to hightlight the relationship between VAEs and DDPM, since this was the process that helped most with my own understanding. 
I have also released a PyTorch implementation of DDPM, with a focus on readability and education.
...


### Variational Autoencoders

I want to begin with a quick refresher of variational autoencoders. If this is all feels familiar to you, feel free to skip ahead!

Consider the following generative model,

$$
  p(x) = \int_{z}p_{\theta}(x|z)p_{\theta}(z)
$$  

In general, the form of the prior $$ p_{\theta}(z) $$ and likelihood $$ p_{\theta}(x|z) $$ will depend on the data we are trying to model.
For simplicity, assume $$ p(z) = \mathcal{N}(0, I) $$. For continous data we can set $$ p_{\theta}(x|z) $$ to a diagonal gaussian,
with entries $$ \mu_{\theta}(z) \text{, } \sigma_{\theta}(z) $$ given by the output of a neural network. 

<!-- If you have studied mixture distributions before (see my [previous post] for an
introduction) this equation will look quite familiar. We have just defined an infinite mixture of gaussians! 
For each value $$ z \sim \mathcal{N}(0, I) $$ there is a corresponding likelihood given by $$ \mathcal{N}(\mu_{\theta}(z), \sigma_{\theta}(z)) $$. -->

Typically the true distribution $$ p(x) $$ is unknown, and we would like to fit the model to some empirically observed subset $$ \hat{p}(x) $$.
We could estimate the model parameters $$ \theta $$ with MLE as follows,

$$
  \theta^{*} = \underset{\theta}{\text{argmax}} \text{ } \mathbb{E}_{x \sim \hat{p}(x), \text{ } z \sim p(z)}  \left[ \text{log } p_{\theta}(x|z; \theta) \right]
$$

However, it is well known that this approach suffers from high variance and does not scale to more than a few dimensions. Intuitively, the likelihood of a random latent
code $$ z \sim p(z) $$ corresponding to any particular data point is extremely small (especially in high dimensions!) (REF). 

The VAE solves this problem by sampling $$ z $$ from a new distribution $$ q_{\phi}(z|x) $$, which is jointly optimised with the generative model. 
This focuses the optimisation on regions of high probability (i.e. latent codes that are likely to have generated $$ x $$).

This can be derived as follows:

$$
  p(x) = \int_{z} p_{\theta}(x|z)p(z) \\
  p(x) = \int q_{\phi}(z|x) \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\\
  \text{log }p(x) = \text{log } \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]\\
  \text{log }p(x) \geq \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \text{log } \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]
$$

The final step follows from Jensen's inequality and the fact that log is a concave function. The right-hand side of this inequality
is the evidence lower-bound (ELBO), sometimes denoted $$ \mathcal{L}(\theta, \phi) $$. The ELBO provides a joint optimisation objective, 
which simultaneously updates the variational posterior $$ q_{\phi}(z|x) $$ and likelihood model $$ p_{\theta}(x|z) $$.

To optimise the ELBO we have to obtain gradients for $$ \phi $$ through a stochastic sampling operation $$ z \sim q_{\phi}(z|x) $$.
Kingma and Welling famously solved this with a "reparameterization trick", which defines the sampling process as a combination of
a deterministic data-dependent function and data-independent noise (REF).

__Figure 1 - Graphical Model for VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/vae.png" }}' height=200px>

So much has been written about VAEs that I am barely scratching the surface here. For more details see: [Further Reading].


### Hierarchical Variational Autoencoders

Having defined a VAE with a single stochastic layer, it is trivial to derive multi-layer extensions.
Consider a VAE with two latent variables $$ z_1 $$ and $$ z_2 $$. We begin by considering the joint 
distribution $$ p(x, z_1, z_2) $$ and marginalising out the latent variables:  

$$
  p(x) = \int_{z_1} \int_{z_2} p(x, z_1, z_2) dz_1, dz_2
$$

Once again, we can introduce a variational posterior as follows:

$$
  p(x) = \int \int q(z_1, z_2|x) \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)}\\
  p(x) = \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Taking the log and applying Jensen's rule:

$$
  \text{log } p(x) \geq \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \text{log } \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Notice how I have written these expressions in terms of the joint distributions 
$$ p(x, z_1, z_2) $$ and $$ q(z_1, z_2|x) $$. I have done this to emphasise the fact that we are free
to factorise the inference and generative models as we see fit. In practice, some factorisations
are more suitable than others [Tomczak reference]. For now lets consider the factorisation in Figure 2.

__Figure 2 - A Hierarchical VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/hierarchical-vae.png" }}' height=200px>

This corresponds to the following factorisation of the generative model,

$$
  p(x,z_1,z_2) = p(x|z_1)p(z_1|z_2)p(z_2)
$$

and the inference model,

$$
  q(z_1,z_2|x) = q(z_1|x)q(z_2|z_1)
$$

Expanding the ELBO for this factorization, we get:

$$
  \mathcal{L}(\theta, \phi) = \mathbb{E}_{q(z_1, z_2|x)} 
  \left[ \text{log }p(x|z_1) - \text{log }q(z_1|x) + \text{log }p(z_1|z_2) - \text{log }q(z_2 | z_1) + \text{log }p(z_2)  \right]\\
$$

Which can be alternatively written as a "reconstruction term", and the KL divergence between each inference layer and its corresponding prior:

$$
= \mathbb{E}_{q(z_1|z_2)} \left[ \text{log }p(x|z_1) \right] - D_{KL} \left( q(z_1|x) \mathrel{\Vert} p(z_1|x) \right)
  - D_{KL} \left( q(z_2|z_1) \mathrel{\Vert} p(z_2) \right)
$$




### Denoising Diffusion Probabilistic Models

In a sense, we can think of DDPM as a specific realisation of a hierarchical VAE with the following properties:
- The latent dimension is constrained to match the data dimension.
- The generative and inference pathways are both factorised as markov chains.
- The inference model has no learned parameters; the form of each layer is wholly determined by the timestep.
- Each generative layer shares parameters and is conditioned on the timestep.
- Expressivity comes from defining a large number of layers.


More specfically, DDPM defines the inference model as a "diffusion process". Beginning with a data point
$$ x_0 ~ p(x) $$, each step of the posterior ("the forward process") adds noise to the previous layer. 
The form of this process is designed so that the final latent $$ x_T $$ converges to a standard gaussian.

Take a look at the visualisation below: 








__Figure 2 - Denoising Diffusion Probabilistic Model__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/diffusion.png" }}' height=200px>


### References

{% include citation.html
    no="1"
    authors="Aaron van den Oord, Sander Dieleman, 
    Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu"
    title="Wavenet: A generative model for raw audio"
    year="2016"
    link="https://arxiv.org/abs/1609.03499v2"
%}

{% if page.comments %}
{% include disqus.html %}
{% endif %}

[pytorch]: https://pytorch.org
[kmeans]: https://en.wikipedia.org/wiki/K-means_clustering
[github]: https://github.com/angusturner/generative_models/
[popgun]: http://popgun.ai/
[variational autoencoders]: https://arxiv.org/abs/1312.6114
[twitter]: https://twitter.com/AngusTurner9
[previous post]: https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html
