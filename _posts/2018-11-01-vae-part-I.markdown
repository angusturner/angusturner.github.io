---
layout: post
title:  "Diffusion Probabilistic Models - Part I"
date:   2021-06-29
categories: generative_models
comments: false
---

{% include mathjax.html %}
{% include styles.html %}

### Introduction

Recently I have been studying a class of generative models known as diffusion probabilistic models. These models were proposed by Sohl-Dickstein 
et al. in 2015 [[1]](#citation-1), however they first caught my attention last year when Ho et al. released 
"Denoising Diffusion Probabilistic Models" [[2]](#citation-2). Building on the earlier work of Sohl-Dickstein et al., 
Ho et al. showed that a model trained with a stable variational objective could match or surpass GANs on image generation.

Since the release of DDPM there has been a wave of renewed interest in diffusion models. New works have extended their success to the domain
of audio modelling [[8]](#citation-8), text-to-speech [[9]]((#citation-9)), and multivariate time-series forecasting [[10]](#citation-10). Furthermore, 
as shown by Ho et al. [[2]](#citation-2), these models exhibit close connections with score-based models [[11]](#citation-11), and the two perspectives 
were recently unified under the framework of stochastic differential equations [[4]](#citation-4). 

As a generative model, diffusion models have a number of unique and interesting properties. For example, trained models are able to
perform inpainting [[1]](#citation-1) and zero-shot denoising [[8]](#citation-8) without being explicitly designed for these tasks. Furthermore,
the variational bound used in DDPM highlights further connections to variational autoencoders and neural compression (Ho et al ICLR).

In this blog I want to explore diffusion models as they are presented in Sohl-Dickstein et al. [[1]](#citation-1), and Ho et al. [[2]](#citation-2).
When I was working through these derivations, I found it useful to conceptualise these models in terms of their relationship to VAEs [[3]](#citation-3). This is 
the perspective I will develop in this blog, working up from a simple VAE derivation, through to hierarchical extensions, and finally the 
generative markov chain that is presented in DDPM.

I have also released a PyTorch implementation of my code...
...


### Variational Autoencoders

I want to begin with a quick refresher of variational autoencoders. If this is all feels familiar to you, feel free to skip ahead!

Consider the following generative model,

$$
  p(x) = \int_{z}p_{\theta}(x|z)p_{\theta}(z)
$$  

In general, the form of the prior $$ p_{\theta}(z) $$ and likelihood $$ p_{\theta}(x|z) $$ will depend on the data we are trying to model.
For simplicity, assume $$ p(z) = \mathcal{N}(0, I) $$. For continous data we can set $$ p_{\theta}(x|z) $$ to a diagonal gaussian,
with entries $$ \mu_{\theta}(z) \text{, } \sigma_{\theta}(z) $$ given by the output of a neural network. 

<!-- If you have studied mixture distributions before (see my [previous post] for an
introduction) this equation will look quite familiar. We have just defined an infinite mixture of gaussians! 
For each value $$ z \sim \mathcal{N}(0, I) $$ there is a corresponding likelihood given by $$ \mathcal{N}(\mu_{\theta}(z), \sigma_{\theta}(z)) $$. -->

Typically the true distribution $$ p(x) $$ is unknown, and we would like to fit the model to some empirically observed subset $$ \hat{p}(x) $$.
We could estimate the model parameters $$ \theta $$ with MLE as follows,

$$
  \theta^{*} = \underset{\theta}{\text{argmax}} \text{ } \mathbb{E}_{x \sim \hat{p}(x) \text{, } z \sim p(z)}  \left[ \text{log } p_{\theta}(x|z; \theta) \right]
$$

Since we cannot compute this expectation in closed form we can resort to monte-carlo sampling, evaluating the expression for random 
samples $$ z \sim p(z) $$. However, it is well known that this approach does not scale to high dimensions of $$ z $$ [[5]](#citation-5).
Intuitively, the likelihood of a random latent code $$ z \sim p(z) $$ corresponding to any particular data point is extremely small
(especially in high dimensions!).

The VAE solves this problem by sampling $$ z $$ from a new distribution $$ q_{\phi}(z|x) $$, which is jointly optimised with the generative model. 
This focuses the optimisation on regions of high probability (i.e. latent codes that are likely to have generated $$ x $$).

This can be derived as follows:

$$
  p(x) = \int_{z} p_{\theta}(x|z)p(z) \\
  p(x) = \int q_{\phi}(z|x) \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\\
  \text{log }p(x) = \text{log } \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]\\
  \text{log }p(x) \geq \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \text{log } \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]
$$

The final step follows from Jensen's inequality and the fact that log is a concave function. The right-hand side of this inequality
is the evidence lower-bound (ELBO), sometimes denoted $$ \mathcal{L}(\theta, \phi) $$. The ELBO provides a joint optimisation objective, 
which simultaneously updates the variational posterior $$ q_{\phi}(z|x) $$ and likelihood model $$ p_{\theta}(x|z) $$.

To optimise the ELBO we have to obtain gradients for $$ \phi $$ through a stochastic sampling operation $$ z \sim q_{\phi}(z|x) $$.
Kingma and Welling famously solved this with a "reparameterization trick", which defines the sampling process as a combination of
a deterministic data-dependent function and data-independent noise [[3]](#citation-3).

__Figure 1 - Graphical Model for VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/vae.png" }}' height=200px>

So much has been written about VAEs that I am barely scratching the surface here. For more details see: [Further Reading].


### Hierarchical Variational Autoencoders

Having defined a VAE with a single stochastic layer, it is trivial to derive multi-layer extensions.
Consider a VAE with two latent variables $$ z_1 $$ and $$ z_2 $$. We begin by considering the joint 
distribution $$ p(x, z_1, z_2) $$ and marginalising out the latent variables:  

$$
  p(x) = \int_{z_1} \int_{z_2} p(x, z_1, z_2) dz_1, dz_2
$$

Once again, we can introduce a variational posterior as follows:

$$
  p(x) = \int \int q(z_1, z_2|x) \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)}\\
  p(x) = \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Taking the log and applying Jensen's rule:

$$
  \text{log } p(x) \geq \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \text{log } \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Notice that I have written these expressions in terms of the joint distributions 
$$ p(x, z_1, z_2) $$ and $$ q(z_1, z_2|x) $$. I have done this to emphasise the fact that we are free
to factorise the inference and generative models as we see fit. In practice, some factorisations
are more suitable than others (as shown in [[7]](#citation-7)). For now lets consider the factorisation in Figure 2.

__Figure 2 - A Hierarchical VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/hierarchical-vae.png" }}' height=200px>

Writing this out, we have the generative model,

$$
  p(x,z_1,z_2) = p(x|z_1)p(z_1|z_2)p(z_2)
$$

and the inference model,

$$
  q(z_1,z_2|x) = q(z_1|x)q(z_2|z_1)
$$

Substituting these factorizations into the ELBO, we get:

$$
  \mathcal{L}(\theta, \phi) = \mathbb{E}_{q(z_1, z_2|x)} 
  \left[ \text{log }p(x|z_1) - \text{log }q(z_1|x) + \text{log }p(z_1|z_2) - \text{log }q(z_2 | z_1) + \text{log }p(z_2)  \right]\\
$$

This can be alternatively written as a "reconstruction term", and the KL divergence between each inference layer and its corresponding prior:

$$
= \mathbb{E}_{q(z_1|z_2)} \left[ \text{log }p(x|z_1) \right] - D_{KL} \left( q(z_1|x) \mathrel{\Vert} p(z_1|x) \right)
  - D_{KL} \left( q(z_2|z_1) \mathrel{\Vert} p(z_2) \right)
$$


### Denoising Diffusion Probabilistic Models

Now consider the model in Figure 3. We have sequence of $$ T $$ variables, where $$ x_0 \sim p(x) $$ is
our observed data and $$ x_{1:T} $$ are latent variables.

__Figure 3 - Denoising Diffusion Probabilistic Model__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/diffusion.png" }}' height=200px>

The lower bound for this model should hopefully seem quite familiar by now:

$$
  \mathcal{L} = \mathbb{E}_{q} \left[ -\text{log }p(x_T) - \sum_{t \geq 1}^{T} \text{log } \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})} \right]
$$

In fact, we can think of DDPM as a specific realisation of a hierarchical VAE. What sets it apart is a
unique inference model, which contains no learnable parameters and is constructed so that
the final latent distribution $$ q(x_T) $$ converges to a standard gaussian. This "forward process"
model is defined as follows:

$$
  q(x_t|x_{t-1}) = \mathcal{N}(x_T\mathrel{;} x_{t-1}\sqrt{1 - \beta_{t}}, \beta_{t}I)
$$

The variables $$ \beta_1 \dots \beta_{T} $$ define a fixed variance schedule, chosen such
that $$ q(x_T|x_0) \approx \mathcal{N}(0, I) $$. The forward process is illustrated below, 
transforming samples from the "swiss roll" distribution (REF) into gaussian noise:

<style>
  .video {
    display: flex;
    margin-left: auto;
    margin-right: auto;
    /* width: 100%; */
    max-height: 500px;
    max-width: 500px;
  }
</style>

<video class="video" loop autoplay preload='auto' poster="/assets/images/fig_000.png">
  <source src='/assets/video/forward_process.mp4' type='video/mp4'>
</video>

A nice property of the forward process is that we can directly sample from any timestep:

$$
  \alpha_t := 1-\beta_t\\
  \hat{\alpha}_t := \prod_{s=1}^{t} \alpha_s \\
  q(x_t|x_0) = \mathcal{N}\left(\sqrt{\hat{\alpha}_t}x_0, (1-\hat{\alpha}_t)I \right)
$$

One consequence of this, is that DDPM is able to draw random samples $$ t ~ \sim \{1 \dots T \} $$ as part of
training procedure, optimising a random set of conditionals $$ p_{\theta} ( x_{t-1} | x_t ) $$ 
with each minibatch. In other words, we do not have to do a complete pass of the forward and reverse processes
on each step of training.

Another consequence, is that we can make additional manipulations to the lower-bound to reduce the variance.
This proceeeds by observing the following result (from bayes rule): 

$$
  q(x_t|x_{t-1}) = \frac{q(x_{t-1}|x_t)q(x_t)}{q(x_{t-1})}
$$

Ordinarily this would be intractable to compute, but due to the markov propety of the forward process
we have:

$$
  q(x_t|x_{t-1}) = q(x_t|x_{t-1},x_0)
$$

Put simply, the distribution on $$ x_{t} $$ is entirely determined by the previous step in the chain;
we do not learn anything extra about the trajectory of a particle by knowing the starting point $$ x_0 $$.
Taking our result from bayes rule, and conditioning on $$ x_0 $$ we get:

$$
  q(x_t|x_{t-1}) = \frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}
$$







DDPM is specific realisation of a hierarchical VAE, with the following properties:
- The generative and inference pathways are both factorised as markov chains.
- The latent dimensionality is constrained to match the data dimensionality.
- The inference pathway has no learned parameters; the form of each layer is wholly determined by its index `t`.
- Each generative layer shares parameters.
- Expressivity comes from defining a chain with many layers (or "timesteps").

<!-- 
More specfically, DDPM defines the inference model as a diffusion process. Beginning with a data point
$$ x_0 \sim p(x) $$, each step of the inference pathway ("forward process") adds noise to the previous layer. The form of 
this process is designed so that the latent distribution $$ p(x_T) $$ converges to a standard gaussian.
To generate data we sample $$ x_T \sim \mathcal{N}(0, I) $$ and transform it through the generative pathway 
("reverse process") to recover a sample from $$ x_0 \sim p(x) $$. -->



### References

{% include citation.html
    no="1"
    authors="Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli"
    title="Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
    year="2015"
    link="https://arxiv.org/abs/1503.03585v8"
%}

{% include citation.html
    no="2"
    authors="Jonathan Ho, Ajay Jain, Pieter Abbeel"
    title="Denoising Diffusion Probabilistic Models"
    year="2020"
    link="https://arxiv.org/abs/2006.11239v2"
%}

{% include citation.html
    no="3"
    authors="Diederik P Kingma, Max Welling"
    title="Auto-Encoding Variational Bayes"
    year="2013"
    link="https://arxiv.org/abs/1312.6114"
%}

{% include citation.html
    no="4"
    authors="Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole"
    title="Score-Based Generative Modeling through Stochastic Differential Equations"
    year="2021"
    link="https://arxiv.org/abs/2011.13456v2"
%}

{% include citation.html
    no="5"
    authors="Adam Kosiorek"
    title="What is wrong with VAEs?"
    year="2018"
    link="https://akosiorek.github.io/ml/2018/03/14/what_is_wrong_with_vaes.html"
%}

{% include citation.html
    no="6"
    authors="Lilian Weng"
    title="From Autoencoder to Beta-VAE"
    year="2018"
    link="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html"
%}

{% include citation.html
    no="7"
    authors="Jakub M. Tomczak, Max Welling"
    title="VAE with a VampPrior"
    year="2018"
    link="https://arxiv.org/abs/1705.07120v5"
%}

{% include citation.html
  no="8"
  authors="Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro"
  title="DiffWave: A Versatile Diffusion Model for Audio Synthesis"
  year="2021"
  link="https://arxiv.org/abs/2009.09761v3"
%}

{% include citation.html
  no="9"
  authors="Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov"
  title="Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech"
  year="2021"
  link="https://arxiv.org/abs/2105.06337v1"
%}

{% include citation.html
  no="10"
  authors="Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf"
  title="Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting"
  year="2021"
  link="https://arxiv.org/abs/2101.12072v2"
%}

{% include citation.html
  no="11"
  authors="Yang Song, Stefano Ermon"
  title="Generative Modeling by Estimating Gradients of the Data Distribution"
  year="2020"
  link="https://arxiv.org/abs/1907.05600v3"
%}


{% if page.comments %}
{% include disqus.html %}
{% endif %}

[pytorch]: https://pytorch.org
[kmeans]: https://en.wikipedia.org/wiki/K-means_clustering
[github]: https://github.com/angusturner/generative_models/
[popgun]: http://popgun.ai/
[variational autoencoders]: https://arxiv.org/abs/1312.6114
[twitter]: https://twitter.com/AngusTurner9
[previous post]: https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html
