---
layout: post
title:  "Diffusion Probabilistic Models - Part I"
date:   2021-06-29
categories: generative_models
comments: false
---

{% include mathjax.html %}
{% include styles.html %}

### Introduction

Recently I have been studying a class of generative models called diffusion models. These models have deep connections to 
variational auto-encoding (ref), neural compression (ref), score-based generative modelling (ref), and stochastic differential equations (ref).
They first caught my attention with the release of "Denoising Diffusion Probabilistic Models" (DDPM) (Ho et al, 2020). This work showed 
that a CNN trained with a variational lower bound is competitive with GANs on image generation. The same insights have subsequently been applied
to audio, produce a competitive neural vocoders which can also perform zero-shot denoising (Kong et al, 2020).

In this blog I want to explore Jonathon Ho's DDPM model from the perpsective of variational inference. This is the perspective 
I am most comfortable with, and I hope that I can build up the theory in an accessible and intuitive way. I will also release a 
corresponding implementation in PyTorch, with a focus on readability and education.
...


### Variational Autoencoders - A Quick Refresher

Consider the following generative model,

$$
  p(x) = \int_{z}p_{\theta}(x|z)p_{\theta}(z)
$$  

In general, the form of the prior $$ p_{\theta}(z) $$ and likelihood $$ p_{\theta}(x|z) $$ will depend on the data we are trying to model.
For simplicity, lets assume a fixed prior of $$ p(z) = \mathcal{N}(0, I) $$. For continous data, we can set $$ p_{\theta}(x|z) $$ to a diagonal gaussian,
with entries $$ \mu_{\theta}(z) \text{, } \sigma_{\theta}(z) $$ given by the output of a neural network. 

If you have studied mixture distributions before (see my [previous post] for an
introduction) this equation will look quite familiar. We have just defined an infinite mixture of gaussians! 
For every possible value $$ z \sim \mathcal{N}(0, I) $$ there is a corresponding likelihood $$ \mathcal{N}(\mu_{\theta}(z), \sigma_{\theta}(z)) $$.

Typically the true distribution $$ p(x) $$ is unknown, and we would like to fit the model to some empirically observed subset $$ \hat{p}(x) $$.
We could estimate the model parameters $$ \theta $$ with MLE as follows,

$$
  \theta^{*} = \underset{\theta}{\text{argmax}} \text{ } \mathbb{E}_{x \sim \hat{p}(x), \text{ } z \sim p(z)}  \left[ \text{log } p_{\theta}(x|z; \theta) \right]
$$

However, it is well known that this approach suffers from high variance and does not scale to more than a few dimensions. Intuitively, the likelihood of a random latent
code $$ z \sim \mathcal{N}(0, I) $$ corresponding to a specific data point $$ x \sim \hat{p}(x) $$ is extremely small (especially in high dimensions!). 
The VAE solves this problem by sampling $$ z $$ from a new distribution $$ q_{\phi}(z|x) $$, which is jointly optimised with the generative model. This focuses the optimisation onto regions of high probability.

This can be derived as follows:

$$
  p(x) = \int p_{\theta}(x|z)p(z) \\
  p(x) = \int q_{\phi}(z|x) \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}\\
  \text{log }p(x) = \text{log } \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]\\
  \text{log }p(x) \geq \mathbb{E}_{z \sim q_{\phi}(z|x)} \left[ \text{log } \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)} \right]
$$

The final step follows from Jensen's inequality and the fact that log is a concave function. The right-hand side of this inequality
is the evidence lower-bound (ELBO), sometimes denoted $$ \mathcal{L}(\theta, \phi) $$. The ELBO provides a joint optimisation objective, 
which simultaneously updates the variational posterior $$ q_{\phi}(z|x) $$ and likelihood model $$ p_{\theta}(x|z) $$.

To optimise the ELBO we have to obtain gradients for $$ \phi $$ through a stochastic sampling operation $$ z \sim q_{\phi}(z|x) $$.
Kingma and Welling famously solved this issue with a "reparameterization trick", which defines the sampling process as a combination of
a deterministic data-dependent function and data-independent noise (REF).

So much has been said about VAEs, that I am barely scratching the surface here. For the interested reader, the following
blogs do a great job of explaining them further: ...

__Figure 1 - Graphical Model for VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/vae.png" }}' height=200px>
<!-- ![VAE]() -->


### Hierarchical Variational Autoencoders

Having defined a VAE with a single stochastic layer, it is trivial to derive multi-layer extensions.
Consider a VAE with two latent variables $$ z_1 $$ and $$ z_2 $$. We begin by considering the joint 
distribution $$ p(x, z_1, z_2) $$ and marginalising out the latent variables:  

$$
  p(x) = \int_{z_1} \int_{z_2} p(x, z_1, z_2) dz_1, dz_2
$$

Once again, we can introduce a variational posterior as follows:

$$
  p(x) = \int \int q(z_1, z_2|x) \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)}\\
  p(x) = \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Taking the log and applying Jensen's rule:

$$
  \text{log } p(x) \geq \mathbb{E}_{z_1 ,z_2 \sim q(z_1, z_2|x)} \left[ \text{log } \frac{p(x, z_1, z_2)}{q(z_1, z_2|x)} \right]
$$

Notice how I have only written these expressions in terms of the joint distributions 
$$ p(x, z_1, z_2) $$ and $$ q(z_1, z_2|x) $$. I have done this to emphasise the fact that we are free
to factorise the inference and generative models as we see fit. In practice, some factorisations
are more suitable than others [Tomczak reference]. For now lets consider the factorisation in Figure 2.

__Figure 2 - A Hierarchical VAE__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/hierarchical-vae.png" }}' height=200px>

This corresponds to the following factorisation of the generative model,

$$
  p(x,z_1,z_2) = p(x|z_1)p(z_1|z_2)p(z_2)
$$

and the inference model,

$$
q(z_1,z_2|x) = q(z_1|x)q(z_2|z_1)
$$

If we expand the ELBO for this factorization, we arrive at the following:

$$

$$


### DDPM as a Restricted Class of Hierarchical VAE


DDPM is a specific realisation of a hierarchical VAE, with the following properties:
- The generative and inference models are both factorised as markov chains.
- The inference model has no learned parameters; The form of each layer is wholly determined by the timestep.
- Each generative layer shares parameters and is conditioned on timestep.
- Expressivity comes from defining a large number of layers.





__Figure 2 - Denoising Diffusion Probabilistic Model__
{: style="text-align: center;" }
<img class="image" src='{{ "/assets/images/diffusion.png" }}' height=200px>


### References

{% include citation.html
    no="1"
    authors="Aaron van den Oord, Sander Dieleman, 
    Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu"
    title="Wavenet: A generative model for raw audio"
    year="2016"
    link="https://arxiv.org/abs/1609.03499v2"
%}

{% if page.comments %}
{% include disqus.html %}
{% endif %}

[pytorch]: https://pytorch.org
[kmeans]: https://en.wikipedia.org/wiki/K-means_clustering
[github]: https://github.com/angusturner/generative_models/
[popgun]: http://popgun.ai/
[variational autoencoders]: https://arxiv.org/abs/1312.6114
[twitter]: https://twitter.com/AngusTurner9
[previous post]: https://angusturner.github.io/generative_models/2017/11/03/pytorch-gaussian-mixture-model.html
